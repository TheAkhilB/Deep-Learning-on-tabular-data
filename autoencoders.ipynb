{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"index.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Creditability</th>\n",
       "      <th>Account Balance</th>\n",
       "      <th>Duration of Credit (month)</th>\n",
       "      <th>Payment Status of Previous Credit</th>\n",
       "      <th>Purpose</th>\n",
       "      <th>Credit Amount</th>\n",
       "      <th>Value Savings/Stocks</th>\n",
       "      <th>Length of current employment</th>\n",
       "      <th>Instalment per cent</th>\n",
       "      <th>Sex &amp; Marital Status</th>\n",
       "      <th>...</th>\n",
       "      <th>Duration in Current address</th>\n",
       "      <th>Most valuable available asset</th>\n",
       "      <th>Age (years)</th>\n",
       "      <th>Concurrent Credits</th>\n",
       "      <th>Type of apartment</th>\n",
       "      <th>No of Credits at this Bank</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>No of dependents</th>\n",
       "      <th>Telephone</th>\n",
       "      <th>Foreign Worker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1049</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2799</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>841</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2122</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2171</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Creditability  Account Balance  Duration of Credit (month)  \\\n",
       "0              1                1                          18   \n",
       "1              1                1                           9   \n",
       "2              1                2                          12   \n",
       "3              1                1                          12   \n",
       "4              1                1                          12   \n",
       "\n",
       "   Payment Status of Previous Credit  Purpose  Credit Amount  \\\n",
       "0                                  4        2           1049   \n",
       "1                                  4        0           2799   \n",
       "2                                  2        9            841   \n",
       "3                                  4        0           2122   \n",
       "4                                  4        0           2171   \n",
       "\n",
       "   Value Savings/Stocks  Length of current employment  Instalment per cent  \\\n",
       "0                     1                             2                    4   \n",
       "1                     1                             3                    2   \n",
       "2                     2                             4                    2   \n",
       "3                     1                             3                    3   \n",
       "4                     1                             3                    4   \n",
       "\n",
       "   Sex & Marital Status  ...  Duration in Current address  \\\n",
       "0                     2  ...                            4   \n",
       "1                     3  ...                            2   \n",
       "2                     2  ...                            4   \n",
       "3                     3  ...                            2   \n",
       "4                     3  ...                            4   \n",
       "\n",
       "   Most valuable available asset  Age (years)  Concurrent Credits  \\\n",
       "0                              2           21                   3   \n",
       "1                              1           36                   3   \n",
       "2                              1           23                   3   \n",
       "3                              1           39                   3   \n",
       "4                              2           38                   1   \n",
       "\n",
       "   Type of apartment  No of Credits at this Bank  Occupation  \\\n",
       "0                  1                           1           3   \n",
       "1                  1                           2           3   \n",
       "2                  1                           1           2   \n",
       "3                  1                           2           2   \n",
       "4                  2                           2           2   \n",
       "\n",
       "   No of dependents  Telephone  Foreign Worker  \n",
       "0                 1          1               1  \n",
       "1                 2          1               1  \n",
       "2                 1          1               1  \n",
       "3                 2          1               2  \n",
       "4                 1          1               2  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns = [\"Creditability\"])\n",
    "y = df['Creditability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=sc.fit_transform(X_train)\n",
    "test_x=sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_x\n",
    "X_test = test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(670, 20)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "encoding_dim = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(input_dim, ))\n",
    "\n",
    "encoder = Dense(encoding_dim, activation=\"tanh\", \n",
    "                activity_regularizer=regularizers.l1(10e-5))(input_layer)\n",
    "encoder = Dense(int(encoding_dim / 2), activation=\"tanh\")(encoder)\n",
    "\n",
    "decoder = Dense(int(encoding_dim / 2), activation='tanh')(encoder)\n",
    "decoder = Dense(input_dim, activation='relu')(decoder)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 670 samples, validate on 330 samples\n",
      "Epoch 1/200\n",
      "670/670 [==============================] - 1s 982us/sample - loss: 1.0732 - accuracy: 0.0269 - val_loss: 1.0376 - val_accuracy: 0.0273\n",
      "Epoch 2/200\n",
      "670/670 [==============================] - 0s 63us/sample - loss: 1.0211 - accuracy: 0.0433 - val_loss: 0.9952 - val_accuracy: 0.0424\n",
      "Epoch 3/200\n",
      "670/670 [==============================] - 0s 68us/sample - loss: 0.9816 - accuracy: 0.0582 - val_loss: 0.9614 - val_accuracy: 0.0636\n",
      "Epoch 4/200\n",
      "670/670 [==============================] - 0s 62us/sample - loss: 0.9489 - accuracy: 0.0836 - val_loss: 0.9328 - val_accuracy: 0.0879\n",
      "Epoch 5/200\n",
      "670/670 [==============================] - 0s 62us/sample - loss: 0.9209 - accuracy: 0.1284 - val_loss: 0.9082 - val_accuracy: 0.1364\n",
      "Epoch 6/200\n",
      "670/670 [==============================] - 0s 63us/sample - loss: 0.8957 - accuracy: 0.1970 - val_loss: 0.8855 - val_accuracy: 0.1818\n",
      "Epoch 7/200\n",
      "670/670 [==============================] - 0s 61us/sample - loss: 0.8722 - accuracy: 0.2478 - val_loss: 0.8641 - val_accuracy: 0.2333\n",
      "Epoch 8/200\n",
      "670/670 [==============================] - 0s 62us/sample - loss: 0.8508 - accuracy: 0.2970 - val_loss: 0.8448 - val_accuracy: 0.2758\n",
      "Epoch 9/200\n",
      "670/670 [==============================] - 0s 67us/sample - loss: 0.8321 - accuracy: 0.3284 - val_loss: 0.8275 - val_accuracy: 0.3030\n",
      "Epoch 10/200\n",
      "670/670 [==============================] - 0s 61us/sample - loss: 0.8147 - accuracy: 0.3597 - val_loss: 0.8122 - val_accuracy: 0.3273\n",
      "Epoch 11/200\n",
      "670/670 [==============================] - 0s 62us/sample - loss: 0.7997 - accuracy: 0.3761 - val_loss: 0.7979 - val_accuracy: 0.3455\n",
      "Epoch 12/200\n",
      "670/670 [==============================] - 0s 57us/sample - loss: 0.7853 - accuracy: 0.3925 - val_loss: 0.7851 - val_accuracy: 0.3788\n",
      "Epoch 13/200\n",
      "670/670 [==============================] - 0s 69us/sample - loss: 0.7723 - accuracy: 0.4119 - val_loss: 0.7730 - val_accuracy: 0.3788\n",
      "Epoch 14/200\n",
      "670/670 [==============================] - 0s 78us/sample - loss: 0.7604 - accuracy: 0.4269 - val_loss: 0.7616 - val_accuracy: 0.4242\n",
      "Epoch 15/200\n",
      "670/670 [==============================] - 0s 77us/sample - loss: 0.7492 - accuracy: 0.4328 - val_loss: 0.7508 - val_accuracy: 0.4364\n",
      "Epoch 16/200\n",
      "670/670 [==============================] - 0s 68us/sample - loss: 0.7384 - accuracy: 0.4537 - val_loss: 0.7407 - val_accuracy: 0.4545\n",
      "Epoch 17/200\n",
      "670/670 [==============================] - 0s 69us/sample - loss: 0.7282 - accuracy: 0.4642 - val_loss: 0.7306 - val_accuracy: 0.4636\n",
      "Epoch 18/200\n",
      "670/670 [==============================] - 0s 64us/sample - loss: 0.7182 - accuracy: 0.4627 - val_loss: 0.7212 - val_accuracy: 0.4727\n",
      "Epoch 19/200\n",
      "670/670 [==============================] - 0s 69us/sample - loss: 0.7086 - accuracy: 0.4657 - val_loss: 0.7119 - val_accuracy: 0.4909\n",
      "Epoch 20/200\n",
      "670/670 [==============================] - 0s 73us/sample - loss: 0.6993 - accuracy: 0.4761 - val_loss: 0.7034 - val_accuracy: 0.5030\n",
      "Epoch 21/200\n",
      "670/670 [==============================] - 0s 77us/sample - loss: 0.6907 - accuracy: 0.4970 - val_loss: 0.6952 - val_accuracy: 0.4970\n",
      "Epoch 22/200\n",
      "670/670 [==============================] - 0s 68us/sample - loss: 0.6823 - accuracy: 0.5060 - val_loss: 0.6879 - val_accuracy: 0.5061\n",
      "Epoch 23/200\n",
      "670/670 [==============================] - 0s 68us/sample - loss: 0.6746 - accuracy: 0.5134 - val_loss: 0.6808 - val_accuracy: 0.5182\n",
      "Epoch 24/200\n",
      "670/670 [==============================] - 0s 69us/sample - loss: 0.6673 - accuracy: 0.5284 - val_loss: 0.6742 - val_accuracy: 0.5212\n",
      "Epoch 25/200\n",
      "670/670 [==============================] - 0s 66us/sample - loss: 0.6604 - accuracy: 0.5299 - val_loss: 0.6679 - val_accuracy: 0.5182\n",
      "Epoch 26/200\n",
      "670/670 [==============================] - 0s 80us/sample - loss: 0.6539 - accuracy: 0.5343 - val_loss: 0.6620 - val_accuracy: 0.5242\n",
      "Epoch 27/200\n",
      "670/670 [==============================] - 0s 68us/sample - loss: 0.6476 - accuracy: 0.5478 - val_loss: 0.6566 - val_accuracy: 0.5152\n",
      "Epoch 28/200\n",
      "670/670 [==============================] - 0s 74us/sample - loss: 0.6419 - accuracy: 0.5567 - val_loss: 0.6512 - val_accuracy: 0.5212\n",
      "Epoch 29/200\n",
      "670/670 [==============================] - 0s 74us/sample - loss: 0.6364 - accuracy: 0.5642 - val_loss: 0.6458 - val_accuracy: 0.5212\n",
      "Epoch 30/200\n",
      "670/670 [==============================] - 0s 76us/sample - loss: 0.6311 - accuracy: 0.5716 - val_loss: 0.6411 - val_accuracy: 0.5333\n",
      "Epoch 31/200\n",
      "670/670 [==============================] - 0s 71us/sample - loss: 0.6260 - accuracy: 0.5806 - val_loss: 0.6367 - val_accuracy: 0.5333\n",
      "Epoch 32/200\n",
      "670/670 [==============================] - 0s 73us/sample - loss: 0.6213 - accuracy: 0.5791 - val_loss: 0.6324 - val_accuracy: 0.5333\n",
      "Epoch 33/200\n",
      "670/670 [==============================] - 0s 69us/sample - loss: 0.6167 - accuracy: 0.5821 - val_loss: 0.6284 - val_accuracy: 0.5394\n",
      "Epoch 34/200\n",
      "670/670 [==============================] - 0s 70us/sample - loss: 0.6124 - accuracy: 0.5896 - val_loss: 0.6246 - val_accuracy: 0.5455\n",
      "Epoch 35/200\n",
      "670/670 [==============================] - 0s 66us/sample - loss: 0.6083 - accuracy: 0.5866 - val_loss: 0.6212 - val_accuracy: 0.5485\n",
      "Epoch 36/200\n",
      "670/670 [==============================] - 0s 109us/sample - loss: 0.6047 - accuracy: 0.5910 - val_loss: 0.6178 - val_accuracy: 0.5515\n",
      "Epoch 37/200\n",
      "670/670 [==============================] - 0s 112us/sample - loss: 0.6009 - accuracy: 0.5896 - val_loss: 0.6145 - val_accuracy: 0.5515\n",
      "Epoch 38/200\n",
      "670/670 [==============================] - 0s 124us/sample - loss: 0.5975 - accuracy: 0.5955 - val_loss: 0.6112 - val_accuracy: 0.5545\n",
      "Epoch 39/200\n",
      "670/670 [==============================] - 0s 133us/sample - loss: 0.5942 - accuracy: 0.5955 - val_loss: 0.6083 - val_accuracy: 0.5576\n",
      "Epoch 40/200\n",
      "670/670 [==============================] - 0s 124us/sample - loss: 0.5912 - accuracy: 0.6000 - val_loss: 0.6057 - val_accuracy: 0.5636\n",
      "Epoch 41/200\n",
      "670/670 [==============================] - 0s 124us/sample - loss: 0.5883 - accuracy: 0.6030 - val_loss: 0.6032 - val_accuracy: 0.5515\n",
      "Epoch 42/200\n",
      "670/670 [==============================] - 0s 112us/sample - loss: 0.5856 - accuracy: 0.6000 - val_loss: 0.6009 - val_accuracy: 0.5515\n",
      "Epoch 43/200\n",
      "670/670 [==============================] - 0s 109us/sample - loss: 0.5833 - accuracy: 0.5985 - val_loss: 0.5988 - val_accuracy: 0.5545\n",
      "Epoch 44/200\n",
      "670/670 [==============================] - 0s 107us/sample - loss: 0.5808 - accuracy: 0.6060 - val_loss: 0.5965 - val_accuracy: 0.5455\n",
      "Epoch 45/200\n",
      "670/670 [==============================] - 0s 112us/sample - loss: 0.5785 - accuracy: 0.6090 - val_loss: 0.5946 - val_accuracy: 0.5455\n",
      "Epoch 46/200\n",
      "670/670 [==============================] - 0s 122us/sample - loss: 0.5763 - accuracy: 0.6134 - val_loss: 0.5926 - val_accuracy: 0.5485\n",
      "Epoch 47/200\n",
      "670/670 [==============================] - 0s 114us/sample - loss: 0.5741 - accuracy: 0.6090 - val_loss: 0.5912 - val_accuracy: 0.5545\n",
      "Epoch 48/200\n",
      "670/670 [==============================] - 0s 118us/sample - loss: 0.5721 - accuracy: 0.6119 - val_loss: 0.5894 - val_accuracy: 0.5606\n",
      "Epoch 49/200\n",
      "670/670 [==============================] - 0s 124us/sample - loss: 0.5702 - accuracy: 0.6269 - val_loss: 0.5879 - val_accuracy: 0.5576\n",
      "Epoch 50/200\n",
      "670/670 [==============================] - 0s 126us/sample - loss: 0.5683 - accuracy: 0.6239 - val_loss: 0.5863 - val_accuracy: 0.5636\n",
      "Epoch 51/200\n",
      "670/670 [==============================] - 0s 116us/sample - loss: 0.5664 - accuracy: 0.6194 - val_loss: 0.5849 - val_accuracy: 0.5545\n",
      "Epoch 52/200\n",
      "670/670 [==============================] - 0s 110us/sample - loss: 0.5647 - accuracy: 0.6224 - val_loss: 0.5833 - val_accuracy: 0.5606\n",
      "Epoch 53/200\n",
      "670/670 [==============================] - 0s 126us/sample - loss: 0.5630 - accuracy: 0.6254 - val_loss: 0.5820 - val_accuracy: 0.5636\n",
      "Epoch 54/200\n",
      "670/670 [==============================] - 0s 127us/sample - loss: 0.5614 - accuracy: 0.6239 - val_loss: 0.5805 - val_accuracy: 0.5606\n",
      "Epoch 55/200\n",
      "670/670 [==============================] - 0s 124us/sample - loss: 0.5598 - accuracy: 0.6313 - val_loss: 0.5793 - val_accuracy: 0.5606\n",
      "Epoch 56/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "670/670 [==============================] - 0s 116us/sample - loss: 0.5583 - accuracy: 0.6284 - val_loss: 0.5781 - val_accuracy: 0.5606\n",
      "Epoch 57/200\n",
      "670/670 [==============================] - 0s 112us/sample - loss: 0.5569 - accuracy: 0.6328 - val_loss: 0.5771 - val_accuracy: 0.5576\n",
      "Epoch 58/200\n",
      "670/670 [==============================] - 0s 120us/sample - loss: 0.5555 - accuracy: 0.6343 - val_loss: 0.5760 - val_accuracy: 0.5545\n",
      "Epoch 59/200\n",
      "670/670 [==============================] - 0s 115us/sample - loss: 0.5541 - accuracy: 0.6373 - val_loss: 0.5750 - val_accuracy: 0.5545\n",
      "Epoch 60/200\n",
      "670/670 [==============================] - 0s 117us/sample - loss: 0.5528 - accuracy: 0.6358 - val_loss: 0.5739 - val_accuracy: 0.5606\n",
      "Epoch 61/200\n",
      "670/670 [==============================] - 0s 115us/sample - loss: 0.5515 - accuracy: 0.6358 - val_loss: 0.5731 - val_accuracy: 0.5636\n",
      "Epoch 62/200\n",
      "670/670 [==============================] - 0s 113us/sample - loss: 0.5503 - accuracy: 0.6373 - val_loss: 0.5716 - val_accuracy: 0.5636\n",
      "Epoch 63/200\n",
      "670/670 [==============================] - 0s 123us/sample - loss: 0.5490 - accuracy: 0.6358 - val_loss: 0.5706 - val_accuracy: 0.5636\n",
      "Epoch 64/200\n",
      "670/670 [==============================] - 0s 115us/sample - loss: 0.5477 - accuracy: 0.6373 - val_loss: 0.5699 - val_accuracy: 0.5636\n",
      "Epoch 65/200\n",
      "670/670 [==============================] - 0s 115us/sample - loss: 0.5466 - accuracy: 0.6448 - val_loss: 0.5690 - val_accuracy: 0.5667\n",
      "Epoch 66/200\n",
      "670/670 [==============================] - 0s 115us/sample - loss: 0.5453 - accuracy: 0.6373 - val_loss: 0.5677 - val_accuracy: 0.5667\n",
      "Epoch 67/200\n",
      "670/670 [==============================] - 0s 120us/sample - loss: 0.5441 - accuracy: 0.6373 - val_loss: 0.5668 - val_accuracy: 0.5667\n",
      "Epoch 68/200\n",
      "670/670 [==============================] - 0s 110us/sample - loss: 0.5430 - accuracy: 0.6388 - val_loss: 0.5661 - val_accuracy: 0.5697\n",
      "Epoch 69/200\n",
      "670/670 [==============================] - 0s 116us/sample - loss: 0.5418 - accuracy: 0.6418 - val_loss: 0.5651 - val_accuracy: 0.5727\n",
      "Epoch 70/200\n",
      "670/670 [==============================] - 0s 114us/sample - loss: 0.5407 - accuracy: 0.6448 - val_loss: 0.5643 - val_accuracy: 0.5667\n",
      "Epoch 71/200\n",
      "670/670 [==============================] - 0s 141us/sample - loss: 0.5396 - accuracy: 0.6448 - val_loss: 0.5631 - val_accuracy: 0.5727\n",
      "Epoch 72/200\n",
      "670/670 [==============================] - 0s 118us/sample - loss: 0.5385 - accuracy: 0.6388 - val_loss: 0.5623 - val_accuracy: 0.5727\n",
      "Epoch 73/200\n",
      "670/670 [==============================] - 0s 116us/sample - loss: 0.5374 - accuracy: 0.6433 - val_loss: 0.5613 - val_accuracy: 0.5697\n",
      "Epoch 74/200\n",
      "670/670 [==============================] - 0s 119us/sample - loss: 0.5363 - accuracy: 0.6463 - val_loss: 0.5602 - val_accuracy: 0.5697\n",
      "Epoch 75/200\n",
      "670/670 [==============================] - 0s 111us/sample - loss: 0.5354 - accuracy: 0.6433 - val_loss: 0.5594 - val_accuracy: 0.5697\n",
      "Epoch 76/200\n",
      "670/670 [==============================] - 0s 113us/sample - loss: 0.5344 - accuracy: 0.6448 - val_loss: 0.5586 - val_accuracy: 0.5758\n",
      "Epoch 77/200\n",
      "670/670 [==============================] - 0s 121us/sample - loss: 0.5334 - accuracy: 0.6463 - val_loss: 0.5577 - val_accuracy: 0.5697\n",
      "Epoch 78/200\n",
      "670/670 [==============================] - 0s 128us/sample - loss: 0.5325 - accuracy: 0.6448 - val_loss: 0.5571 - val_accuracy: 0.5758\n",
      "Epoch 79/200\n",
      "670/670 [==============================] - 0s 120us/sample - loss: 0.5316 - accuracy: 0.6478 - val_loss: 0.5560 - val_accuracy: 0.5818\n",
      "Epoch 80/200\n",
      "670/670 [==============================] - 0s 119us/sample - loss: 0.5308 - accuracy: 0.6493 - val_loss: 0.5551 - val_accuracy: 0.5818\n",
      "Epoch 81/200\n",
      "670/670 [==============================] - 0s 111us/sample - loss: 0.5298 - accuracy: 0.6463 - val_loss: 0.5543 - val_accuracy: 0.5818\n",
      "Epoch 82/200\n",
      "670/670 [==============================] - 0s 115us/sample - loss: 0.5290 - accuracy: 0.6463 - val_loss: 0.5535 - val_accuracy: 0.5879\n",
      "Epoch 83/200\n",
      "670/670 [==============================] - 0s 120us/sample - loss: 0.5281 - accuracy: 0.6522 - val_loss: 0.5528 - val_accuracy: 0.5879\n",
      "Epoch 84/200\n",
      "670/670 [==============================] - 0s 115us/sample - loss: 0.5273 - accuracy: 0.6507 - val_loss: 0.5521 - val_accuracy: 0.5909\n",
      "Epoch 85/200\n",
      "670/670 [==============================] - 0s 124us/sample - loss: 0.5264 - accuracy: 0.6522 - val_loss: 0.5510 - val_accuracy: 0.5939\n",
      "Epoch 86/200\n",
      "670/670 [==============================] - 0s 115us/sample - loss: 0.5254 - accuracy: 0.6493 - val_loss: 0.5504 - val_accuracy: 0.5970\n",
      "Epoch 87/200\n",
      "670/670 [==============================] - 0s 118us/sample - loss: 0.5245 - accuracy: 0.6493 - val_loss: 0.5493 - val_accuracy: 0.5939\n",
      "Epoch 88/200\n",
      "670/670 [==============================] - 0s 125us/sample - loss: 0.5237 - accuracy: 0.6522 - val_loss: 0.5485 - val_accuracy: 0.5909\n",
      "Epoch 89/200\n",
      "670/670 [==============================] - 0s 113us/sample - loss: 0.5228 - accuracy: 0.6597 - val_loss: 0.5479 - val_accuracy: 0.5970\n",
      "Epoch 90/200\n",
      "670/670 [==============================] - 0s 116us/sample - loss: 0.5220 - accuracy: 0.6537 - val_loss: 0.5470 - val_accuracy: 0.5970\n",
      "Epoch 91/200\n",
      "670/670 [==============================] - 0s 125us/sample - loss: 0.5211 - accuracy: 0.6567 - val_loss: 0.5462 - val_accuracy: 0.6061\n",
      "Epoch 92/200\n",
      "670/670 [==============================] - 0s 121us/sample - loss: 0.5204 - accuracy: 0.6537 - val_loss: 0.5453 - val_accuracy: 0.6091\n",
      "Epoch 93/200\n",
      "670/670 [==============================] - 0s 115us/sample - loss: 0.5196 - accuracy: 0.6567 - val_loss: 0.5447 - val_accuracy: 0.6030\n",
      "Epoch 94/200\n",
      "670/670 [==============================] - 0s 115us/sample - loss: 0.5189 - accuracy: 0.6493 - val_loss: 0.5436 - val_accuracy: 0.6121\n",
      "Epoch 95/200\n",
      "670/670 [==============================] - 0s 118us/sample - loss: 0.5181 - accuracy: 0.6537 - val_loss: 0.5430 - val_accuracy: 0.6030\n",
      "Epoch 96/200\n",
      "670/670 [==============================] - 0s 119us/sample - loss: 0.5175 - accuracy: 0.6537 - val_loss: 0.5421 - val_accuracy: 0.6182\n",
      "Epoch 97/200\n",
      "670/670 [==============================] - 0s 114us/sample - loss: 0.5167 - accuracy: 0.6567 - val_loss: 0.5413 - val_accuracy: 0.6212\n",
      "Epoch 98/200\n",
      "670/670 [==============================] - 0s 117us/sample - loss: 0.5159 - accuracy: 0.6567 - val_loss: 0.5407 - val_accuracy: 0.6182\n",
      "Epoch 99/200\n",
      "670/670 [==============================] - 0s 118us/sample - loss: 0.5153 - accuracy: 0.6552 - val_loss: 0.5397 - val_accuracy: 0.6273\n",
      "Epoch 100/200\n",
      "670/670 [==============================] - 0s 111us/sample - loss: 0.5145 - accuracy: 0.6612 - val_loss: 0.5391 - val_accuracy: 0.6212\n",
      "Epoch 101/200\n",
      "670/670 [==============================] - 0s 113us/sample - loss: 0.5138 - accuracy: 0.6597 - val_loss: 0.5382 - val_accuracy: 0.6212\n",
      "Epoch 102/200\n",
      "670/670 [==============================] - 0s 116us/sample - loss: 0.5130 - accuracy: 0.6597 - val_loss: 0.5374 - val_accuracy: 0.6182\n",
      "Epoch 103/200\n",
      "670/670 [==============================] - 0s 116us/sample - loss: 0.5124 - accuracy: 0.6567 - val_loss: 0.5366 - val_accuracy: 0.6212\n",
      "Epoch 104/200\n",
      "670/670 [==============================] - 0s 113us/sample - loss: 0.5118 - accuracy: 0.6597 - val_loss: 0.5361 - val_accuracy: 0.6303\n",
      "Epoch 105/200\n",
      "670/670 [==============================] - 0s 128us/sample - loss: 0.5111 - accuracy: 0.6627 - val_loss: 0.5349 - val_accuracy: 0.6273\n",
      "Epoch 106/200\n",
      "670/670 [==============================] - 0s 119us/sample - loss: 0.5103 - accuracy: 0.6582 - val_loss: 0.5342 - val_accuracy: 0.6273\n",
      "Epoch 107/200\n",
      "670/670 [==============================] - 0s 118us/sample - loss: 0.5095 - accuracy: 0.6642 - val_loss: 0.5338 - val_accuracy: 0.6394\n",
      "Epoch 108/200\n",
      "670/670 [==============================] - 0s 115us/sample - loss: 0.5089 - accuracy: 0.6597 - val_loss: 0.5328 - val_accuracy: 0.6333\n",
      "Epoch 109/200\n",
      "670/670 [==============================] - 0s 110us/sample - loss: 0.5081 - accuracy: 0.6746 - val_loss: 0.5317 - val_accuracy: 0.6364\n",
      "Epoch 110/200\n",
      "670/670 [==============================] - 0s 116us/sample - loss: 0.5074 - accuracy: 0.6746 - val_loss: 0.5313 - val_accuracy: 0.6364\n",
      "Epoch 111/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "670/670 [==============================] - 0s 119us/sample - loss: 0.5066 - accuracy: 0.6687 - val_loss: 0.5302 - val_accuracy: 0.6364\n",
      "Epoch 112/200\n",
      "670/670 [==============================] - 0s 113us/sample - loss: 0.5059 - accuracy: 0.6821 - val_loss: 0.5295 - val_accuracy: 0.6394\n",
      "Epoch 113/200\n",
      "670/670 [==============================] - 0s 117us/sample - loss: 0.5052 - accuracy: 0.6866 - val_loss: 0.5291 - val_accuracy: 0.6333\n",
      "Epoch 114/200\n",
      "670/670 [==============================] - 0s 120us/sample - loss: 0.5046 - accuracy: 0.6806 - val_loss: 0.5283 - val_accuracy: 0.6364\n",
      "Epoch 115/200\n",
      "670/670 [==============================] - 0s 128us/sample - loss: 0.5038 - accuracy: 0.6836 - val_loss: 0.5281 - val_accuracy: 0.6364\n",
      "Epoch 116/200\n",
      "670/670 [==============================] - 0s 130us/sample - loss: 0.5032 - accuracy: 0.6836 - val_loss: 0.5273 - val_accuracy: 0.6394\n",
      "Epoch 117/200\n",
      "670/670 [==============================] - 0s 122us/sample - loss: 0.5025 - accuracy: 0.6866 - val_loss: 0.5268 - val_accuracy: 0.6394\n",
      "Epoch 118/200\n",
      "670/670 [==============================] - 0s 119us/sample - loss: 0.5020 - accuracy: 0.6866 - val_loss: 0.5258 - val_accuracy: 0.6424\n",
      "Epoch 119/200\n",
      "670/670 [==============================] - 0s 114us/sample - loss: 0.5013 - accuracy: 0.6866 - val_loss: 0.5255 - val_accuracy: 0.6394\n",
      "Epoch 120/200\n",
      "670/670 [==============================] - 0s 117us/sample - loss: 0.5009 - accuracy: 0.6881 - val_loss: 0.5254 - val_accuracy: 0.6424\n",
      "Epoch 121/200\n",
      "670/670 [==============================] - 0s 123us/sample - loss: 0.5002 - accuracy: 0.6791 - val_loss: 0.5246 - val_accuracy: 0.6394\n",
      "Epoch 122/200\n",
      "670/670 [==============================] - 0s 113us/sample - loss: 0.4995 - accuracy: 0.6896 - val_loss: 0.5237 - val_accuracy: 0.6515\n",
      "Epoch 123/200\n",
      "670/670 [==============================] - 0s 110us/sample - loss: 0.4991 - accuracy: 0.6925 - val_loss: 0.5233 - val_accuracy: 0.6576\n",
      "Epoch 124/200\n",
      "670/670 [==============================] - 0s 109us/sample - loss: 0.4983 - accuracy: 0.6955 - val_loss: 0.5229 - val_accuracy: 0.6333\n",
      "Epoch 125/200\n",
      "670/670 [==============================] - 0s 114us/sample - loss: 0.4978 - accuracy: 0.7060 - val_loss: 0.5219 - val_accuracy: 0.6485\n",
      "Epoch 126/200\n",
      "670/670 [==============================] - 0s 114us/sample - loss: 0.4972 - accuracy: 0.6925 - val_loss: 0.5216 - val_accuracy: 0.6424\n",
      "Epoch 127/200\n",
      "670/670 [==============================] - 0s 122us/sample - loss: 0.4966 - accuracy: 0.6940 - val_loss: 0.5211 - val_accuracy: 0.6455\n",
      "Epoch 128/200\n",
      "670/670 [==============================] - 0s 110us/sample - loss: 0.4960 - accuracy: 0.6985 - val_loss: 0.5207 - val_accuracy: 0.6545\n",
      "Epoch 129/200\n",
      "670/670 [==============================] - 0s 118us/sample - loss: 0.4954 - accuracy: 0.6985 - val_loss: 0.5204 - val_accuracy: 0.6515\n",
      "Epoch 130/200\n",
      "670/670 [==============================] - 0s 120us/sample - loss: 0.4948 - accuracy: 0.6955 - val_loss: 0.5194 - val_accuracy: 0.6545\n",
      "Epoch 131/200\n",
      "670/670 [==============================] - 0s 113us/sample - loss: 0.4940 - accuracy: 0.6970 - val_loss: 0.5188 - val_accuracy: 0.6515\n",
      "Epoch 132/200\n",
      "670/670 [==============================] - 0s 117us/sample - loss: 0.4934 - accuracy: 0.7000 - val_loss: 0.5184 - val_accuracy: 0.6394\n",
      "Epoch 133/200\n",
      "670/670 [==============================] - 0s 126us/sample - loss: 0.4928 - accuracy: 0.7030 - val_loss: 0.5176 - val_accuracy: 0.6394\n",
      "Epoch 134/200\n",
      "670/670 [==============================] - 0s 114us/sample - loss: 0.4922 - accuracy: 0.7075 - val_loss: 0.5171 - val_accuracy: 0.6515\n",
      "Epoch 135/200\n",
      "670/670 [==============================] - 0s 124us/sample - loss: 0.4916 - accuracy: 0.7119 - val_loss: 0.5169 - val_accuracy: 0.6515\n",
      "Epoch 136/200\n",
      "670/670 [==============================] - 0s 113us/sample - loss: 0.4910 - accuracy: 0.7090 - val_loss: 0.5161 - val_accuracy: 0.6576\n",
      "Epoch 137/200\n",
      "670/670 [==============================] - 0s 121us/sample - loss: 0.4903 - accuracy: 0.7104 - val_loss: 0.5152 - val_accuracy: 0.6606\n",
      "Epoch 138/200\n",
      "670/670 [==============================] - 0s 118us/sample - loss: 0.4898 - accuracy: 0.7209 - val_loss: 0.5147 - val_accuracy: 0.6606\n",
      "Epoch 139/200\n",
      "670/670 [==============================] - 0s 117us/sample - loss: 0.4891 - accuracy: 0.7119 - val_loss: 0.5141 - val_accuracy: 0.6515\n",
      "Epoch 140/200\n",
      "670/670 [==============================] - 0s 118us/sample - loss: 0.4885 - accuracy: 0.7119 - val_loss: 0.5134 - val_accuracy: 0.6515\n",
      "Epoch 141/200\n",
      "670/670 [==============================] - 0s 113us/sample - loss: 0.4880 - accuracy: 0.7149 - val_loss: 0.5130 - val_accuracy: 0.6515\n",
      "Epoch 142/200\n",
      "670/670 [==============================] - 0s 109us/sample - loss: 0.4874 - accuracy: 0.7164 - val_loss: 0.5123 - val_accuracy: 0.6606\n",
      "Epoch 143/200\n",
      "670/670 [==============================] - 0s 117us/sample - loss: 0.4868 - accuracy: 0.7149 - val_loss: 0.5118 - val_accuracy: 0.6667\n",
      "Epoch 144/200\n",
      "670/670 [==============================] - 0s 125us/sample - loss: 0.4862 - accuracy: 0.7164 - val_loss: 0.5112 - val_accuracy: 0.6667\n",
      "Epoch 145/200\n",
      "670/670 [==============================] - 0s 124us/sample - loss: 0.4857 - accuracy: 0.7164 - val_loss: 0.5107 - val_accuracy: 0.6636\n",
      "Epoch 146/200\n",
      "670/670 [==============================] - 0s 136us/sample - loss: 0.4853 - accuracy: 0.7209 - val_loss: 0.5098 - val_accuracy: 0.6667\n",
      "Epoch 147/200\n",
      "670/670 [==============================] - 0s 116us/sample - loss: 0.4846 - accuracy: 0.7179 - val_loss: 0.5093 - val_accuracy: 0.6636\n",
      "Epoch 148/200\n",
      "670/670 [==============================] - 0s 111us/sample - loss: 0.4841 - accuracy: 0.7164 - val_loss: 0.5083 - val_accuracy: 0.6636\n",
      "Epoch 149/200\n",
      "670/670 [==============================] - 0s 128us/sample - loss: 0.4836 - accuracy: 0.7239 - val_loss: 0.5082 - val_accuracy: 0.6697\n",
      "Epoch 150/200\n",
      "670/670 [==============================] - 0s 129us/sample - loss: 0.4832 - accuracy: 0.7179 - val_loss: 0.5074 - val_accuracy: 0.6576\n",
      "Epoch 151/200\n",
      "670/670 [==============================] - 0s 124us/sample - loss: 0.4829 - accuracy: 0.7194 - val_loss: 0.5070 - val_accuracy: 0.6667\n",
      "Epoch 152/200\n",
      "670/670 [==============================] - 0s 128us/sample - loss: 0.4823 - accuracy: 0.7164 - val_loss: 0.5065 - val_accuracy: 0.6727\n",
      "Epoch 153/200\n",
      "670/670 [==============================] - 0s 118us/sample - loss: 0.4819 - accuracy: 0.7194 - val_loss: 0.5056 - val_accuracy: 0.6758\n",
      "Epoch 154/200\n",
      "670/670 [==============================] - 0s 126us/sample - loss: 0.4814 - accuracy: 0.7149 - val_loss: 0.5052 - val_accuracy: 0.6758\n",
      "Epoch 155/200\n",
      "670/670 [==============================] - 0s 119us/sample - loss: 0.4810 - accuracy: 0.7164 - val_loss: 0.5042 - val_accuracy: 0.6788\n",
      "Epoch 156/200\n",
      "670/670 [==============================] - 0s 119us/sample - loss: 0.4803 - accuracy: 0.7239 - val_loss: 0.5037 - val_accuracy: 0.6879\n",
      "Epoch 157/200\n",
      "670/670 [==============================] - 0s 116us/sample - loss: 0.4797 - accuracy: 0.7239 - val_loss: 0.5036 - val_accuracy: 0.6758\n",
      "Epoch 158/200\n",
      "670/670 [==============================] - 0s 115us/sample - loss: 0.4795 - accuracy: 0.7239 - val_loss: 0.5029 - val_accuracy: 0.6848\n",
      "Epoch 159/200\n",
      "670/670 [==============================] - 0s 118us/sample - loss: 0.4789 - accuracy: 0.7254 - val_loss: 0.5025 - val_accuracy: 0.6818\n",
      "Epoch 160/200\n",
      "670/670 [==============================] - 0s 118us/sample - loss: 0.4784 - accuracy: 0.7209 - val_loss: 0.5018 - val_accuracy: 0.6848\n",
      "Epoch 161/200\n",
      "670/670 [==============================] - 0s 117us/sample - loss: 0.4778 - accuracy: 0.7254 - val_loss: 0.5012 - val_accuracy: 0.6879\n",
      "Epoch 162/200\n",
      "670/670 [==============================] - 0s 121us/sample - loss: 0.4776 - accuracy: 0.7239 - val_loss: 0.5011 - val_accuracy: 0.6848\n",
      "Epoch 163/200\n",
      "670/670 [==============================] - 0s 116us/sample - loss: 0.4770 - accuracy: 0.7269 - val_loss: 0.5006 - val_accuracy: 0.6818\n",
      "Epoch 164/200\n",
      "670/670 [==============================] - 0s 119us/sample - loss: 0.4765 - accuracy: 0.7239 - val_loss: 0.5001 - val_accuracy: 0.6848\n",
      "Epoch 165/200\n",
      "670/670 [==============================] - 0s 123us/sample - loss: 0.4761 - accuracy: 0.7269 - val_loss: 0.4997 - val_accuracy: 0.6788\n",
      "Epoch 166/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "670/670 [==============================] - 0s 114us/sample - loss: 0.4757 - accuracy: 0.7209 - val_loss: 0.4986 - val_accuracy: 0.6909\n",
      "Epoch 167/200\n",
      "670/670 [==============================] - 0s 68us/sample - loss: 0.4753 - accuracy: 0.7284 - val_loss: 0.4992 - val_accuracy: 0.6848\n",
      "Epoch 168/200\n",
      "670/670 [==============================] - 0s 114us/sample - loss: 0.4750 - accuracy: 0.7313 - val_loss: 0.4981 - val_accuracy: 0.6818\n",
      "Epoch 169/200\n",
      "670/670 [==============================] - 0s 77us/sample - loss: 0.4746 - accuracy: 0.7224 - val_loss: 0.4981 - val_accuracy: 0.6818\n",
      "Epoch 170/200\n",
      "670/670 [==============================] - 0s 123us/sample - loss: 0.4740 - accuracy: 0.7313 - val_loss: 0.4974 - val_accuracy: 0.6909\n",
      "Epoch 171/200\n",
      "670/670 [==============================] - 0s 124us/sample - loss: 0.4735 - accuracy: 0.7328 - val_loss: 0.4970 - val_accuracy: 0.6879\n",
      "Epoch 172/200\n",
      "670/670 [==============================] - 0s 118us/sample - loss: 0.4731 - accuracy: 0.7269 - val_loss: 0.4969 - val_accuracy: 0.6939\n",
      "Epoch 173/200\n",
      "670/670 [==============================] - 0s 118us/sample - loss: 0.4726 - accuracy: 0.7313 - val_loss: 0.4963 - val_accuracy: 0.6879\n",
      "Epoch 174/200\n",
      "670/670 [==============================] - 0s 124us/sample - loss: 0.4721 - accuracy: 0.7343 - val_loss: 0.4960 - val_accuracy: 0.6879\n",
      "Epoch 175/200\n",
      "670/670 [==============================] - 0s 114us/sample - loss: 0.4718 - accuracy: 0.7358 - val_loss: 0.4953 - val_accuracy: 0.6970\n",
      "Epoch 176/200\n",
      "670/670 [==============================] - 0s 118us/sample - loss: 0.4713 - accuracy: 0.7358 - val_loss: 0.4951 - val_accuracy: 0.7030\n",
      "Epoch 177/200\n",
      "670/670 [==============================] - 0s 127us/sample - loss: 0.4708 - accuracy: 0.7358 - val_loss: 0.4943 - val_accuracy: 0.6939\n",
      "Epoch 178/200\n",
      "670/670 [==============================] - 0s 123us/sample - loss: 0.4705 - accuracy: 0.7343 - val_loss: 0.4943 - val_accuracy: 0.6879\n",
      "Epoch 179/200\n",
      "670/670 [==============================] - 0s 125us/sample - loss: 0.4700 - accuracy: 0.7343 - val_loss: 0.4932 - val_accuracy: 0.6909\n",
      "Epoch 180/200\n",
      "670/670 [==============================] - 0s 124us/sample - loss: 0.4694 - accuracy: 0.7403 - val_loss: 0.4928 - val_accuracy: 0.6939\n",
      "Epoch 181/200\n",
      "670/670 [==============================] - 0s 115us/sample - loss: 0.4689 - accuracy: 0.7433 - val_loss: 0.4922 - val_accuracy: 0.6939\n",
      "Epoch 182/200\n",
      "670/670 [==============================] - 0s 122us/sample - loss: 0.4685 - accuracy: 0.7433 - val_loss: 0.4920 - val_accuracy: 0.7091\n",
      "Epoch 183/200\n",
      "670/670 [==============================] - 0s 106us/sample - loss: 0.4680 - accuracy: 0.7463 - val_loss: 0.4912 - val_accuracy: 0.7000\n",
      "Epoch 184/200\n",
      "670/670 [==============================] - 0s 111us/sample - loss: 0.4674 - accuracy: 0.7507 - val_loss: 0.4906 - val_accuracy: 0.7000\n",
      "Epoch 185/200\n",
      "670/670 [==============================] - 0s 108us/sample - loss: 0.4669 - accuracy: 0.7478 - val_loss: 0.4902 - val_accuracy: 0.6970\n",
      "Epoch 186/200\n",
      "670/670 [==============================] - 0s 120us/sample - loss: 0.4664 - accuracy: 0.7478 - val_loss: 0.4899 - val_accuracy: 0.7000\n",
      "Epoch 187/200\n",
      "670/670 [==============================] - 0s 127us/sample - loss: 0.4659 - accuracy: 0.7537 - val_loss: 0.4889 - val_accuracy: 0.7121\n",
      "Epoch 188/200\n",
      "670/670 [==============================] - 0s 67us/sample - loss: 0.4653 - accuracy: 0.7478 - val_loss: 0.4889 - val_accuracy: 0.7030\n",
      "Epoch 189/200\n",
      "670/670 [==============================] - 0s 118us/sample - loss: 0.4648 - accuracy: 0.7493 - val_loss: 0.4880 - val_accuracy: 0.7152\n",
      "Epoch 190/200\n",
      "670/670 [==============================] - 0s 125us/sample - loss: 0.4644 - accuracy: 0.7612 - val_loss: 0.4880 - val_accuracy: 0.7242\n",
      "Epoch 191/200\n",
      "670/670 [==============================] - 0s 115us/sample - loss: 0.4638 - accuracy: 0.7612 - val_loss: 0.4866 - val_accuracy: 0.7242\n",
      "Epoch 192/200\n",
      "670/670 [==============================] - 0s 70us/sample - loss: 0.4634 - accuracy: 0.7537 - val_loss: 0.4867 - val_accuracy: 0.7242\n",
      "Epoch 193/200\n",
      "670/670 [==============================] - 0s 120us/sample - loss: 0.4627 - accuracy: 0.7582 - val_loss: 0.4856 - val_accuracy: 0.7242\n",
      "Epoch 194/200\n",
      "670/670 [==============================] - 0s 126us/sample - loss: 0.4623 - accuracy: 0.7597 - val_loss: 0.4851 - val_accuracy: 0.7212\n",
      "Epoch 195/200\n",
      "670/670 [==============================] - 0s 121us/sample - loss: 0.4618 - accuracy: 0.7642 - val_loss: 0.4848 - val_accuracy: 0.7394\n",
      "Epoch 196/200\n",
      "670/670 [==============================] - 0s 113us/sample - loss: 0.4614 - accuracy: 0.7627 - val_loss: 0.4842 - val_accuracy: 0.7394\n",
      "Epoch 197/200\n",
      "670/670 [==============================] - 0s 115us/sample - loss: 0.4609 - accuracy: 0.7627 - val_loss: 0.4839 - val_accuracy: 0.7333\n",
      "Epoch 198/200\n",
      "670/670 [==============================] - 0s 105us/sample - loss: 0.4604 - accuracy: 0.7672 - val_loss: 0.4828 - val_accuracy: 0.7424\n",
      "Epoch 199/200\n",
      "670/670 [==============================] - 0s 112us/sample - loss: 0.4600 - accuracy: 0.7582 - val_loss: 0.4826 - val_accuracy: 0.7394\n",
      "Epoch 200/200\n",
      "670/670 [==============================] - 0s 103us/sample - loss: 0.4595 - accuracy: 0.7642 - val_loss: 0.4817 - val_accuracy: 0.7424\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 200\n",
    "batch_size = 64\n",
    "\n",
    "autoencoder.compile(optimizer='adam', \n",
    "                    loss='mean_squared_error', \n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "history = autoencoder.fit(X_train, X_train,\n",
    "                    epochs=nb_epoch,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(X_test, X_test),\n",
    "                    verbose=1,\n",
    "                    callbacks=[checkpointer, tensorboard]).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = autoencoder.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = np.mean(np.power(X_test - predictions, 2), axis=1)\n",
    "error_df = pd.DataFrame({'reconstruction_error': mse,\n",
    "                        'true_class': y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 2.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = [\"Score\", \"NoScore\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = [1 if e > threshold else 0 for e in error_df.reconstruction_error.values]\n",
    "conf_matrix = confusion_matrix(error_df.true_class, y_pred)\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.ylabel('True class')\n",
    "plt.xlabel('Predicted class')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
